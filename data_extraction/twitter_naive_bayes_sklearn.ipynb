{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier sentiment analysis with scikit-learn\n",
    "\n",
    "In this notebook, a Naive Bayes Classifier is used to classify french and german tweets. In the provided data, a sentiment analysis has already been done on the french and german tweets, but only using the smileys. Now, the classified tweets are used to classify the other tweets that do not have these smileys and that have been classified as neutral. One problem is that we do not have tweets that have been classified as neural BECAUSE they are neutral. Indeed, most neutral tweets are in fact unclassified tweets. Because of class imbalances, we only consider subsets of positive tweets.\n",
    "\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer, GermanStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the snowball algorithm (http://snowballstem.org/) to stem french and german words. A casual tokenizer is used as it is more adapted to Twitter data. Stopwords are also loaded for both languages. A simple regular expression is compiled to detect links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_stemmer = FrenchStemmer()\n",
    "de_stemmer = GermanStemmer()\n",
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "stop = stopwords.words('french') + stopwords.words('german')\n",
    "re_url = re.compile('^(http[s]?://)?[\\S]+\\.\\S[\\S]+[/?#][\\S]+$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define some useful function to make the code easier to read\n",
    "\n",
    "Remove stopwords, URLs and single character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_word(word):\n",
    "    return word not in stop and not re_url.match(word) and len(word) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem th word after removing a possible hashsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_word(stemmer, word):\n",
    "    return stemmer.stem(word.replace('#', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of lists of items to a list of items (2D -> 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return [item for sublist in lst for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a tweet into a list of cleaned, stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(x, stemmer):\n",
    "    return ' '.join(clean_word(stemmer, word) for word in tokenizer.tokenize(x) if filter_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse one month of data from a CSV file for a given language. Tweets are separated by sentiment in three different Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_month(curr_pos, curr_neg, curr_neu, lang, month):\n",
    "    raw = pd.read_csv('../processed/tweets_non-en_msg_{}.csv'.format(month), escapechar='\\\\')\n",
    "    raw.columns=['source_location', 'lang', 'main', 'sentiment']\n",
    "\n",
    "    filtered = raw[raw.lang == lang]\n",
    "\n",
    "    pos = filtered[filtered.sentiment == 'POSITIVE']\n",
    "    neg = filtered[filtered.sentiment == 'NEGATIVE']\n",
    "    neu = filtered[filtered.sentiment == 'NEUTRAL']\n",
    "    \n",
    "    stemmer = fr_stemmer if lang == 'fr' else de_stemmer if lang == 'de' else None\n",
    "\n",
    "    pos = pos[['source_location', 'sentiment']].assign(tokenized=pos['main'].apply(lambda x: process(x, stemmer)))\n",
    "    neg = neg[['source_location', 'sentiment']].assign(tokenized=neg['main'].apply(lambda x: process(x, stemmer)))\n",
    "    neu = neu[['source_location', 'sentiment']].assign(tokenized=neu['main'].apply(lambda x: process(x, stemmer)))\n",
    "    \n",
    "    if curr_pos is None:\n",
    "        return (pos, neg, neu)\n",
    "    else:\n",
    "        return (pd.concat([curr_pos, pos], copy=False),\n",
    "                pd.concat([curr_neg, neg], copy=False),\n",
    "                pd.concat([curr_neu, neu], copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all the neutral tweets and give them the label POSITIVE or NEGATIVE. If a tweet does not contain words found in the training set, then give it the NEUTRAL label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_neutral_tweets(classifier, X_train, neutral):\n",
    "    # We need to find what features were found in the tweet. Training this at each function call\n",
    "    # is OVERKILL. But it doesn't slow the process that much -> to be improved in the future.\n",
    "    count_vec = CountVectorizer()\n",
    "    fit = count_vec.fit(X_train)\n",
    "    features = count_vec.transform(neutral.tokenized.tolist())\n",
    "    \n",
    "    # Classify tweets\n",
    "    predicted = classifier.predict(neutral.tokenized.tolist())\n",
    "    neutral = neutral.assign(sentiment=predicted)\n",
    "    \n",
    "    # Remove unclassifiable tweets (their features is the zero matrix)\n",
    "    stay_neutral = np.array(features.sum(axis=1) == 0).squeeze()\n",
    "    neutral['sentiment'][stay_neutral] = 'NEUTRAL'\n",
    "    \n",
    "    return neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the tweets for the given months. For the best resulsts, cross-validation and grid search are used. Grid search is done on the number of n-grams, the utilization of idf and the bayes smoothing paramters. See the comments in the following cell for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiments_by_month(dataset, lang, months):\n",
    "    X = dataset.tokenized.tolist()\n",
    "    y = dataset.sentiment.tolist()\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "    \n",
    "    # Create a classifier pipeline\n",
    "    clf = Pipeline([('count_vec', CountVectorizer()), ('tfidf', TfidfTransformer()), ('bayes', MultinomialNB())])\n",
    "    \n",
    "    # Set the grid search parameters\n",
    "    parameters = {'count_vec__ngram_range': [(1, 1), (1, 2), (1, 3), (1, 4)],\n",
    "                  'tfidf__use_idf': (False, True),\n",
    "                  'bayes__alpha': (1, 1e-1, 1e-2, 1e-3)}\n",
    "    \n",
    "    # Do grid search with 5-fold cross-validation\n",
    "    clf_gs = GridSearchCV(clf, parameters, cv=5, n_jobs=3)\n",
    "    clf_gs.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Accuracy: {}\".format(round(clf_gs.best_score_, 3)))\n",
    "    \n",
    "    # Compute and plot the confusion matrix\n",
    "    predicted = clf_gs.predict(X_test)\n",
    "    labels = ['NEGATIVE', 'POSITIVE']\n",
    "    matrix = metrics.confusion_matrix(y_test, predicted, labels=labels)\n",
    "    \n",
    "    print(\"Confusion matrix:\")\n",
    "    plt.figure()\n",
    "    plt.imshow(matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.xticks([0, 1], labels, rotation=45)\n",
    "    plt.yticks([0, 1], labels)\n",
    "    \n",
    "    # Write the number of elements in each cell\n",
    "    thresh = matrix.max() / 2.\n",
    "    for i, j in itertools.product(range(matrix.shape[0]), range(matrix.shape[1])):\n",
    "        plt.text(j, i, matrix[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if matrix[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Compute output for all months\n",
    "    output = {}\n",
    "    \n",
    "    for month in months:\n",
    "        print(\"Classifying month {}...\".format(month))\n",
    "        positive, negative, neutral = parse_month(None, None, None, lang, month)\n",
    "        \n",
    "        # Predicted tweets are concatenated with already classified tweets\n",
    "        output[month] = pd.concat([positive, negative, classify_neutral_tweets(clf_gs, X_train, neutral)])[['source_location', 'sentiment']]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the CSVs. Store the tweets of different languages in different DataFrames. The `<lang>_all` DataFrames contains positive and negative tweets for a specific language. `<neu>_all` contain neutral tweets. Empty tweets (after processing) are removed from these DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare variables to store the data for each language and sentiment\n",
    "fr_pos = None\n",
    "fr_neg = None\n",
    "fr_neu = None\n",
    "de_pos = None\n",
    "de_neg = None\n",
    "de_neu = None\n",
    "\n",
    "# The months we have in the data\n",
    "months = [\n",
    "    'january',\n",
    "    'february',\n",
    "    'march',\n",
    "    'april',\n",
    "    'may',\n",
    "    'june',\n",
    "    'july',\n",
    "    'august',\n",
    "    'september',\n",
    "    'october'\n",
    "]\n",
    "\n",
    "# fr\n",
    "for month in months:\n",
    "    fr_pos, fr_neg, fr_neu = parse_month(fr_pos, fr_neg, fr_neu, 'fr', month)\n",
    "\n",
    "fr_pos = fr_pos[0:len(fr_neg)]                          # Undersample due to class imbalance\n",
    "fr_all = pd.concat([fr_pos, fr_neg], copy=False)        # Build labeled set\n",
    "fr_all = fr_all[fr_all.tokenized != '']                 # Remove empty strings\n",
    "fr_neu = fr_neu[fr_neu.tokenized != '']\n",
    "\n",
    "# de\n",
    "for month in months:\n",
    "    de_pos, de_neg, de_neu = parse_month(de_pos, de_neg, de_neu, 'de', month)\n",
    "\n",
    "de_pos = de_pos[0:len(de_neg)]\n",
    "de_all = pd.concat([de_pos, de_neg], copy=False)\n",
    "de_all = de_all[de_all.tokenized != '']\n",
    "de_neu = de_neu[de_neu.tokenized != '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data that will be used to train the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delémont</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>arrêt foutr merd 2016 svp merc beaucoup bon nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Baden</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>nous bier pros neujahr ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>on calcul le gen réveillent mainten alor bon a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>ça manqu peut-êtr peu poes ça mani d'optimis p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>bon anné coral te compliqu trop vi ;)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source_location sentiment  \\\n",
       "4           Delémont  POSITIVE   \n",
       "414            Baden  POSITIVE   \n",
       "1366        Lausanne  POSITIVE   \n",
       "1439        Lausanne  POSITIVE   \n",
       "1467        Lausanne  POSITIVE   \n",
       "\n",
       "                                              tokenized  \n",
       "4     arrêt foutr merd 2016 svp merc beaucoup bon nu...  \n",
       "414                           nous bier pros neujahr ;)  \n",
       "1366  on calcul le gen réveillent mainten alor bon a...  \n",
       "1439  ça manqu peut-êtr peu poes ça mani d'optimis p...  \n",
       "1467              bon anné coral te compliqu trop vi ;)  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Zuerich</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>so start neu jahr :-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Biel/Bienne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>ja chonnt mach ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Solothurn</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>solang stolp fall fuhrt gewiss rein ;-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Biel/Bienne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>wir hoff ihr hattet gut start neu jahr ;-) wir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Chur</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>es gab schnee jahr :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_location sentiment  \\\n",
       "41          Zuerich  POSITIVE   \n",
       "50      Biel/Bienne  POSITIVE   \n",
       "134       Solothurn  POSITIVE   \n",
       "180     Biel/Bienne  POSITIVE   \n",
       "216            Chur  POSITIVE   \n",
       "\n",
       "                                             tokenized  \n",
       "41                               so start neu jahr :-)  \n",
       "50                                   ja chonnt mach ;)  \n",
       "134            solang stolp fall fuhrt gewiss rein ;-)  \n",
       "180  wir hoff ihr hattet gut start neu jahr ;-) wir...  \n",
       "216                              es gab schnee jahr :)  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to store the results per month. Classify the tweets and keep only source_location and sentiment, per month. We also plot the accuracy as well as the confusion matrix to estimate how well the classifiers perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.76\n",
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASMAAAEcCAYAAACMDNJVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc070d27b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying month january...\n",
      "Classifying month february...\n",
      "Classifying month march...\n",
      "Classifying month april...\n",
      "Classifying month may...\n",
      "Classifying month june...\n",
      "Classifying month july...\n",
      "Classifying month august...\n"
     ]
    }
   ],
   "source": [
    "fr_by_month = sentiments_by_month(fr_all, 'fr', months)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the data to CSV that will be loaded on the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['source_location', 'sentiment']\n",
    "path = '../processed/sentiments/tweets_sentiments_bayes_fr_{}.csv'\n",
    "\n",
    "for key, df in fr_by_month.items():\n",
    "    f = open(path.format(key), 'w')\n",
    "    f.write(pd.DataFrame(df.groupby(columns).size()).to_csv())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for german tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "de_by_month = sentiments_by_month(de_all, 'de', months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['source_location', 'sentiment']\n",
    "path = '../processed/sentiments/tweets_sentiments_bayes_de_{}.csv'\n",
    "\n",
    "for key, df in de_by_month.items():\n",
    "    f = open(path.format(key), 'w')\n",
    "    f.write(pd.DataFrame(df.groupby(columns).size()).to_csv())\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
