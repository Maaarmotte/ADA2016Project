{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier sentiment analysis with scikit-learn\n",
    "\n",
    "In this notebook, a Naive Bayes Classifier is used to classify french and german tweets. In the provided data, a sentiment analysis has already been done on the french and german tweets, but only using the smileys. Now, they classified tweets are used to classify the other tweets that do not have these smileys and that have been classified as neutral. One problem is that we do not have tweets that have been classified as neural BECAUSE they are neutral. Indeed, most neutral tweets are in fact unclassified tweets. Because of class imbalances, we only consider subsets of positive tweets.\n",
    "\n",
    "Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk.stem.snowball import FrenchStemmer, GermanStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import f1_score, recall_score, accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the snowball algorithm (http://snowballstem.org/) to stem french and german words. A casual tokenizer is used as it is more adapted to Twitter data. Stopwords are also loaded for both languages. A simple regular expression is compiled to detect links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_stemmer = FrenchStemmer()\n",
    "de_stemmer = GermanStemmer()\n",
    "tokenizer = TweetTokenizer(strip_handles=True)\n",
    "stop = stopwords.words('french') + stopwords.words('german')\n",
    "re_url = re.compile('^(http[s]?://)?[\\S]+\\.\\S[\\S]+[/?#][\\S]+$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define some useful function to make the code easier to read\n",
    "\n",
    "Remove stopwords, URLs and single character words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filter_word(word):\n",
    "    return word not in stop and not re_url.match(word) and len(word) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem th word after removing a possible hashsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_word(stemmer, word):\n",
    "    return stemmer.stem(word.replace('#', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert a list of lists of items to a list of items (2D -> 1D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(lst):\n",
    "    return [item for sublist in lst for item in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform a tweet into a list of cleaned, stemmed words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process(x, stemmer):\n",
    "    return ' '.join(clean_word(stemmer, word) for word in tokenizer.tokenize(x) if filter_word(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generate another function that will be used to find what features are present in a tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#def generate_callback(features):\n",
    "#    def extract_features(text):\n",
    "#        extract = {}#[None]*len(features)\n",
    "#        for feature in features:\n",
    "#            extract[feature] = feature in text\n",
    "#        return extract\n",
    "    \n",
    "#    return extract_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse one month of data from a CSV file for a given language. Tweets are separated by sentiment in three different Pandas DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_month(curr_pos, curr_neg, curr_neu, lang, month):\n",
    "    raw = pd.read_csv('processed/tweets_non-en_msg_{}.csv'.format(month), escapechar='\\\\')\n",
    "    raw.columns=['source_location', 'lang', 'main', 'sentiment']\n",
    "\n",
    "    filtered = raw[raw.lang == lang]\n",
    "\n",
    "    pos = filtered[filtered.sentiment == 'POSITIVE']\n",
    "    neg = filtered[filtered.sentiment == 'NEGATIVE']\n",
    "    neu = filtered[filtered.sentiment == 'NEUTRAL']\n",
    "    \n",
    "    stemmer = fr_stemmer if lang == 'fr' else de_stemmer if lang == 'de' else None\n",
    "\n",
    "    pos = pos[['source_location', 'sentiment']].assign(tokenized=pos['main'].apply(lambda x: process(x, stemmer)))\n",
    "    neg = neg[['source_location', 'sentiment']].assign(tokenized=neg['main'].apply(lambda x: process(x, stemmer)))\n",
    "    neu = neu[['source_location', 'sentiment']].assign(tokenized=neu['main'].apply(lambda x: process(x, stemmer)))\n",
    "    \n",
    "    if curr_pos is None:\n",
    "        return (pos, neg, neu)\n",
    "    else:\n",
    "        return (pd.concat([curr_pos, pos], copy=False),\n",
    "                pd.concat([curr_neg, neg], copy=False),\n",
    "                pd.concat([curr_neu, neu], copy=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Naive Bays classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_classifier(X_train, y_train):\n",
    "    count_vec = CountVectorizer()\n",
    "    X_features = count_vec.fit_transform(X_train)\n",
    "    \n",
    "    tf_transformer = TfidfTransformer(use_idf=False)\n",
    "    X_tf = tf_transformer.fit_transform(X_features)\n",
    "    \n",
    "    return (MultinomialNB().fit(X_tf, y_train), count_vec, tf_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the accuracy, recall and f1 score (based on accuracy and recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def score(classifier, count_vec, tf_transformer, X_test, y_test):\n",
    "    # Binarize and test\n",
    "    binary = preprocessing.LabelBinarizer()\n",
    "    y_pred, features = predict(classifier, count_vec, tf_transformer, X_test)\n",
    "    \n",
    "    y_test = binary.fit_transform(y_test)\n",
    "    y_pred = binary.fit_transform(y_pred.tolist())\n",
    "    \n",
    "    print(\"Accuracy: {}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"Recall: {}\".format(recall_score(y_test, y_pred)))\n",
    "    print(\"F1 Score: {}\".format(f1_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict using the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(classifier, count_vec, tf_transformer, X):\n",
    "    X_new_features = count_vec.transform(X)\n",
    "    X_new_tf = tf_transformer.transform(X_new_features)\n",
    "    return classifier.predict(X_new_tf), X_new_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the neutral tweets using the positive and negative tweets that were classified using the smileys (:), :(). Note that tweets for which the features are all zeros will stay neutral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_neutral_tweets(classifier, count_vec, tf_transformer, neutral):\n",
    "    # Classify tweets\n",
    "    predicted, features = predict(classifier, count_vec, tf_transformer, neutral.tokenized.tolist())\n",
    "    neutral = neutral.assign(sentiment=predicted)\n",
    "    \n",
    "    # Remove unclassifiable tweets (their features is the zero matrix)\n",
    "    stay_neutral = np.array(features.sum(axis=1) == 0).squeeze()\n",
    "    neutral['sentiment'][stay_neutral] = 'NEUTRAL'\n",
    "    \n",
    "    return neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classify the tweets for the given months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiments_by_month(dataset, lang, months):\n",
    "    X = dataset.tokenized.tolist()\n",
    "    y = dataset.sentiment.tolist()\n",
    "\n",
    "    # Split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n",
    "\n",
    "    # Train\n",
    "    classifier, count_vec, tf = fit_classifier(X_train, y_train)\n",
    "    \n",
    "    # Test\n",
    "    score(classifier, count_vec, tf, X_test, y_test)\n",
    "\n",
    "    output = {}\n",
    "    \n",
    "    for month in months:\n",
    "        print(\"Classifying month {}...\".format(month))\n",
    "        positive, negative, neutral = parse_month(None, None, None, lang, month)\n",
    "        output[month] = pd.concat([positive, negative, classify_neutral_tweets(classifier, count_vec, tf, neutral)])[['source_location', 'sentiment']]\n",
    "        \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from the CSVs. Store the tweets of different languages in different DataFrames. The <lang>_all DataFrames contains positive and negative tweets for a specific language. Empty tweets (after processing) are removed from these DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fr_pos = None\n",
    "fr_neg = None\n",
    "fr_neu = None\n",
    "de_pos = None\n",
    "de_neg = None\n",
    "de_neu = None\n",
    "\n",
    "months = [\n",
    "    'january',\n",
    "    'february',\n",
    "    'march',\n",
    "    'april',\n",
    "    'may',\n",
    "    'june',\n",
    "    'july',\n",
    "    'august',\n",
    "    'september',\n",
    "    'october'\n",
    "]\n",
    "\n",
    "# fr\n",
    "for month in months:\n",
    "    fr_pos, fr_neg, fr_neu = parse_month(fr_pos, fr_neg, fr_neu, 'fr', month)\n",
    "\n",
    "#fr_pos = fr_pos.sample(frac=1).reset_index(drop=True)   # Shuffle the DataFrame\n",
    "fr_pos = fr_pos[0:len(fr_neg)]                          # Take only a subset due to class imbalance\n",
    "fr_all = pd.concat([fr_pos, fr_neg], copy=False)\n",
    "fr_all = fr_all[fr_all.tokenized != '']                 # Remove empty strings\n",
    "fr_neu = fr_neu[fr_neu.tokenized != '']\n",
    "\n",
    "# de\n",
    "for month in months:\n",
    "    de_pos, de_neg, de_neu = parse_month(de_pos, de_neg, de_neu, 'de', month)\n",
    "\n",
    "#de_pos = de_pos.sample(frac=1).reset_index(drop=True)\n",
    "de_pos = de_pos[0:len(de_neg)]\n",
    "de_all = pd.concat([de_pos, de_neg], copy=False)\n",
    "de_all = de_all[de_all.tokenized != '']\n",
    "de_neu = de_neu[de_neu.tokenized != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Delémont</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>arrêt foutr merd 2016 svp merc beaucoup bon nu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>Baden</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>nous bier pros neujahr ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1366</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>on calcul le gen réveillent mainten alor bon a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1439</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>ça manqu peut-êtr peu poes ça mani d'optimis p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>Lausanne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>bon anné coral te compliqu trop vi ;)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source_location sentiment  \\\n",
       "4           Delémont  POSITIVE   \n",
       "414            Baden  POSITIVE   \n",
       "1366        Lausanne  POSITIVE   \n",
       "1439        Lausanne  POSITIVE   \n",
       "1467        Lausanne  POSITIVE   \n",
       "\n",
       "                                              tokenized  \n",
       "4     arrêt foutr merd 2016 svp merc beaucoup bon nu...  \n",
       "414                           nous bier pros neujahr ;)  \n",
       "1366  on calcul le gen réveillent mainten alor bon a...  \n",
       "1439  ça manqu peut-êtr peu poes ça mani d'optimis p...  \n",
       "1467              bon anné coral te compliqu trop vi ;)  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fr_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_location</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Zuerich</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>so start neu jahr :-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>Biel/Bienne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>ja chonnt mach ;)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>Solothurn</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>solang stolp fall fuhrt gewiss rein ;-)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>Biel/Bienne</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>wir hoff ihr hattet gut start neu jahr ;-) wir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>Chur</td>\n",
       "      <td>POSITIVE</td>\n",
       "      <td>es gab schnee jahr :)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_location sentiment  \\\n",
       "41          Zuerich  POSITIVE   \n",
       "50      Biel/Bienne  POSITIVE   \n",
       "134       Solothurn  POSITIVE   \n",
       "180     Biel/Bienne  POSITIVE   \n",
       "216            Chur  POSITIVE   \n",
       "\n",
       "                                             tokenized  \n",
       "41                               so start neu jahr :-)  \n",
       "50                                   ja chonnt mach ;)  \n",
       "134            solang stolp fall fuhrt gewiss rein ;-)  \n",
       "180  wir hoff ihr hattet gut start neu jahr ;-) wir...  \n",
       "216                              es gab schnee jahr :)  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to store the results per month. Classify the tweets and keep only source_location and sentiment, per month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7512165450121655\n",
      "Recall: 0.6970443349753694\n",
      "F1 Score: 0.734587929915639\n",
      "Classifying month january...\n",
      "Classifying month february...\n",
      "Classifying month march...\n",
      "Classifying month april...\n",
      "Classifying month may...\n",
      "Classifying month june...\n",
      "Classifying month july...\n",
      "Classifying month august...\n",
      "Classifying month september...\n",
      "Classifying month october...\n"
     ]
    }
   ],
   "source": [
    "fr_by_month = sentiments_by_month(fr_all, 'fr', months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns = ['source_location', 'sentiment']\n",
    "path = 'processed/sentiments/tweets_sentiments_bayes_fr_{}.csv'\n",
    "\n",
    "for key, df in fr_by_month.items():\n",
    "    f = open(path.format(key), 'w')\n",
    "    f.write(pd.DataFrame(df.groupby(columns).size()).to_csv())\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7178683385579937\n",
      "Recall: 0.7100213219616205\n",
      "F1 Score: 0.7115384615384616\n",
      "Classifying month january...\n",
      "Classifying month february...\n",
      "Classifying month march...\n",
      "Classifying month april...\n",
      "Classifying month may...\n",
      "Classifying month june...\n",
      "Classifying month july...\n",
      "Classifying month august...\n",
      "Classifying month september...\n",
      "Classifying month october...\n"
     ]
    }
   ],
   "source": [
    "de_by_month = sentiments_by_month(de_all, 'de', months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['source_location', 'sentiment']\n",
    "path = 'processed/sentiments/tweets_sentiments_bayes_de_{}.csv'\n",
    "\n",
    "for key, df in de_by_month.items():\n",
    "    f = open(path.format(key), 'w')\n",
    "    f.write(pd.DataFrame(df.groupby(columns).size()).to_csv())\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
